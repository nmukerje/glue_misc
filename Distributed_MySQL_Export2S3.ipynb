{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Database Tables to S3 in Parquet.\n",
    "\n",
    "This notebook demonstrates how to export multiple database tables in parallel to Parquet files in S3 using Spark.\n",
    "\n",
    "Note:\n",
    "1. Package dependencies are - s3fs, pandas, pyarrow, pymysql/pg8000.\n",
    "2. This code uses pymysql. For Postgres, one can use pg8000.\n",
    "3. Output files will have a max records set to say 10M which keeps container memory demands within control and parquet file sizes within recommended range - 128 MB to 1GB.\n",
    "4. For timestamp based exports, one can use timestamp column as filters in the SQL queries that exports the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:56:31.649263Z",
     "start_time": "2019-04-30T03:56:31.632671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2.4.0'"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:56:29.611535Z",
     "start_time": "2019-04-30T03:56:28.881206Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from s3fs import S3FileSystem\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def connect2DB(host,user,password,db):\n",
    "    ''' connect to Database '''\n",
    "    # Connect to the database\n",
    "    db = pymysql.connect(host=host,\n",
    "                         user=user,\n",
    "                         password=password,\n",
    "                         db=db,\n",
    "                         charset='utf8mb4',\n",
    "                         cursorclass=pymysql.cursors.DictCursor)\n",
    "    return db\n",
    "\n",
    "def runSQL(sql,limit=10):\n",
    "    ''' executes SQL statement '''\n",
    "    result=[]\n",
    "    cur.execute(sql)\n",
    "    for i,row in enumerate(cur):\n",
    "        if i > limit:\n",
    "            break\n",
    "        result.append(row) \n",
    "    return result;\n",
    "\n",
    "def export2S3(host,user,password,db,tablename,s3Location):\n",
    "    ''' exports file to S3 '''\n",
    "    # Max records per file = chunk size\n",
    "    chunk_size = 50000\n",
    "    offset = 0\n",
    "    s3 = S3FileSystem() \n",
    "    conn=connect2DB(host,user,password,db)\n",
    "    while True:\n",
    "        sql=\"Select * from %s limit %d offset %d\"%(tablename,chunk_size,offset)\n",
    "        df=pd.read_sql(sql,conn)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_to_dataset(table, s3Location, filesystem=s3, use_dictionary=True, compression='snappy')\n",
    "        offset += chunk_size\n",
    "        if df.shape[0] < chunk_size:\n",
    "            break\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:58:58.290934Z",
     "start_time": "2019-04-30T03:58:58.067163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUSTOMER', 'CUSTOMER_DIM', 'CUSTOMER_SITE', 'PRODUCT', 'PRODUCT_CATEGORY', 'PRODUCT_DIM', 'SALES_ORDER', 'SALES_ORDER_ALL', 'SALES_ORDER_DETAIL', 'SALES_ORDER_FACT', 'SALES_ORDER_V']"
     ]
    }
   ],
   "source": [
    "host='aurora-3.abc.us-west-2.rds.amazonaws.com'\n",
    "user='<user>'\n",
    "password='<password>'\n",
    "db='<schema>'\n",
    "bucket='<bucket>'\n",
    "\n",
    "# connect to database\n",
    "conn=connect2DB(host,user,password,db)\n",
    "cur = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "# get tables in database\n",
    "output=runSQL(\"Show tables\")\n",
    "\n",
    "tables=[list(t.values())[0] for t in output]\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:59:35.069246Z",
     "start_time": "2019-04-30T03:59:25.807434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True]"
     ]
    }
   ],
   "source": [
    "df=sc.parallelize(tables).map(lambda x:Row(x)).toDF([\"table\"])\n",
    "# repartition to get max parallelism from Spark cluster\n",
    "df=df.repartition(10)\n",
    "df.rdd.map(lambda x:export2S3(host,user,password,db,x['table'],\\\n",
    "                              's3://{0}/mysql/{1}/{2}'.format(bucket,db,x['table']))).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
