{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "\n",
    "Let's generate some data to build our datalake. We will generate a SALES_ORDER_FACT dataset in S3 in a version enabled bucket with year/month/day/hour partitions. We will generate ~100-128MB parquet files per hour for a year of data i.e. ~1 TB of data across 8760 files.\n",
    "\n",
    "Package Dependencies to generate the data:\n",
    "\n",
    "* Pandas\n",
    "* Pyarrow\n",
    "* S3FS\n",
    "\n",
    "This was run on a 300-unit EMR Instance Fleet with a mix of R4 and R5 Spot instances with the above python packages bootstrapped on to the cluster. Total run time should be less than 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:27:41.595717Z",
     "start_time": "2019-05-15T22:27:23.321299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1562004426807_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-34-149.us-west-2.compute.internal:20888/proxy/application_1562004426807_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-40-116.us-west-2.compute.internal:8042/node/containerlogs/container_1562004426807_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '8000M', 'executorMemory': '8000M', 'executorCores': 1, 'numExecutors': 152, 'conf': {'spark.executor.memoryOverhead': '2G'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1562004426807_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-34-149.us-west-2.compute.internal:20888/proxy/application_1562004426807_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-41-123.us-west-2.compute.internal:8042/node/containerlogs/container_1562004426807_0006_01_000001/livy\">Link</a></td><td></td></tr><tr><td>6</td><td>application_1562004426807_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-34-149.us-west-2.compute.internal:20888/proxy/application_1562004426807_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-40-116.us-west-2.compute.internal:8042/node/containerlogs/container_1562004426807_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"8000M\",\"executorMemory\": \"8000M\", \"executorCores\": 1, \"numExecutors\":152, \"conf\":  { \"spark.executor.memoryOverhead\":\"2G\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *gen_order* function generates an Order record with some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:59:50.895821Z",
     "start_time": "2019-05-15T23:59:50.674414Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import datetime, random\n",
    "from random import randrange\n",
    "from pyspark.sql import Row\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from s3fs import S3FileSystem\n",
    "import os\n",
    "import string\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "def gen_order(f,order_id,i,startDate):\n",
    "    site_id=random.randint(1,500)\n",
    "    order_date=startDate\n",
    "    ship_modes=['STANDARD','ONE-DAY','TWO-DAY','NO-RUSH']\n",
    "    ship_mode=ship_modes[random.randint(0,3)]\n",
    "    last_modified_timestamp=startDate + datetime.timedelta(seconds=randrange(86400))\n",
    "    lines=random.randint(1,5)\n",
    "    for k in range(lines):\n",
    "        line_id=k+1\n",
    "        line_number=k+1\n",
    "        product_id=random.randint(0,1000)\n",
    "        quantity=random.randint(0,100)\n",
    "        unit_price=random.randint(0,1000)/1\n",
    "        supply_cost=unit_price/random.uniform(0.1, 5.0)/1\n",
    "        discount=unit_price/random.uniform(0.1, 5.0)/1\n",
    "        tax=unit_price/random.uniform(0.1, 5.0)/1\n",
    "        f.write(\"{\"+f'\"ORDER_ID\": {order_id}, \"SITE_ID\": {site_id}, \\\n",
    "\"ORDER_DATE\": \"{order_date.isoformat()}\", \\\n",
    "\"SHIP_MODE\": \"{ship_mode}\", \"LINE_ID\": {line_id}, \"LINE_NUMBER\": {line_number},\\\n",
    "\"PRODUCT_ID\": {product_id}, \"QUANTITY\": {quantity}, \"UNIT_PRICE\": {unit_price}, \\\n",
    "\"DISCOUNT\": {discount}, \"SUPPLY_COST\": {supply_cost}, \"TAX\": {tax}, \\\n",
    "\"LAST_MODIFIED_TIMESTAMP\": \"{last_modified_timestamp.isoformat()}\"'+\"}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "startDate = datetime.datetime(2018, 1, 1,0,0) to start from Jan 1st, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:28:14.738863Z",
     "start_time": "2019-05-15T22:28:14.720858Z"
    }
   },
   "outputs": [],
   "source": [
    "#create new local file.\n",
    "recordsPerFile=1031500\n",
    "def drop_file(i):\n",
    "    startDate = datetime.datetime(2018, 1, 1,0,0)+ datetime.timedelta(hours=i)\n",
    "    filename='/tmp/'+id_generator()+'.txt'\n",
    "    with open(filename, 'w+') as f:\n",
    "        for k in range(i*recordsPerFile,(i+1)*recordsPerFile):\n",
    "            gen_order(f,k,i,startDate)\n",
    "    return (filename,startDate)\n",
    "\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:28:17.024931Z",
     "start_time": "2019-05-15T22:28:17.006179Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket='neilawsversionedo'\n",
    "t='SALES_ORDER_FACT'\n",
    "\n",
    "# Place file in S3 datalake\n",
    "def generate_file(i):\n",
    "    f,startDate=drop_file(i)\n",
    "    df=pd.read_json(f,lines=True)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    s3Location='s3://{0}/cdc/{1}/year={2}/month={3}/day={4}/hour={5}'\\\n",
    "        .format(bucket,t,startDate.year,startDate.month,startDate.day,startDate.hour)\n",
    "    s3 = S3FileSystem() \n",
    "    pq.write_to_dataset(table, s3Location, filesystem=s3, use_dictionary=True, compression='snappy')\n",
    "    os.remove(f)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n is Number of hours to generate data = number of days * 24 e.g. (31+30+31) * 24 = 2208 for 3 months of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:36:20.714248Z",
     "start_time": "2019-05-15T22:36:19.481828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   i|\n",
      "+----+\n",
      "|2680|\n",
      "|1844|\n",
      "| 584|\n",
      "|2469|\n",
      "|1019|\n",
      "|3130|\n",
      "| 616|\n",
      "| 917|\n",
      "|2675|\n",
      "| 652|\n",
      "+----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "n=8760\n",
    "df=sc.parallelize(range(n)).map(lambda x:Row(x)).toDF([\"i\"])\n",
    "df=df.repartition(n)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:44:44.526258Z",
     "start_time": "2019-05-15T22:36:23.801652Z"
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.map(lambda x:generate_file(x[\"i\"])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
