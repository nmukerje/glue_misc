{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "\n",
    "Let's generate some data to build our datalake. We will generate a SALES_ORDER_FACT dataset in S3 in a version enabled bucket with year/month/day/hour partitions.\n",
    "\n",
    "Package Dependencies to generate the data:\n",
    "\n",
    "* Pandas\n",
    "* Pyarrow\n",
    "* S3FS\n",
    "\n",
    "This was run a 2 node r5.2xlarge instance with the above python packages boostrapped on to the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:27:41.595717Z",
     "start_time": "2019-05-15T22:27:23.321299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>156</td><td>application_1555125850663_0160</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-13.us-west-2.compute.internal:20888/proxy/application_1555125850663_0160/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-91.us-west-2.compute.internal:8042/node/containerlogs/container_1555125850663_0160_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '8000M', 'executorMemory': '8000M', 'executorCores': 1, 'numExecutors': 20, 'conf': {'spark.executor.memoryOverhead': '2G'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>156</td><td>application_1555125850663_0160</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-13.us-west-2.compute.internal:20888/proxy/application_1555125850663_0160/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-91.us-west-2.compute.internal:8042/node/containerlogs/container_1555125850663_0160_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"8000M\",\"executorMemory\": \"8000M\", \"executorCores\": 1, \"numExecutors\":20, \"conf\":  { \"spark.executor.memoryOverhead\":\"2G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:59:50.895821Z",
     "start_time": "2019-05-15T23:59:50.674414Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import datetime, random\n",
    "from random import randrange\n",
    "from pyspark.sql import Row\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from s3fs import S3FileSystem\n",
    "import os\n",
    "import string\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "def gen_order(f,order_id,i,startDate):\n",
    "    site_id=random.randint(1,500)\n",
    "    order_date=startDate\n",
    "    ship_modes=['STANDARD','ONE-DAY','TWO-DAY','NO-RUSH']\n",
    "    ship_mode=ship_modes[random.randint(0,3)]\n",
    "    last_modified_timestamp=startDate + datetime.timedelta(seconds=randrange(86400))\n",
    "    lines=random.randint(1,5)\n",
    "    for k in range(lines):\n",
    "        line_id=k+1\n",
    "        line_number=k+1\n",
    "        product_id=random.randint(0,1000)\n",
    "        quantity=random.randint(0,100)\n",
    "        unit_price=random.randint(0,1000)/1\n",
    "        supply_cost=unit_price/random.uniform(0.1, 5.0)/1\n",
    "        discount=unit_price/random.uniform(0.1, 5.0)/1\n",
    "        tax=unit_price/random.uniform(0.1, 5.0)/1\n",
    "        f.write(\"{\"+f'\"ORDER_ID\": {order_id}, \"SITE_ID\": {site_id}, \\\n",
    "\"ORDER_DATE\": \"{order_date.isoformat()}\", \\\n",
    "\"SHIP_MODE\": \"{ship_mode}\", \"LINE_ID\": {line_id}, \"LINE_NUMBER\": {line_number},\\\n",
    "\"PRODUCT_ID\": {product_id}, \"QUANTITY\": {quantity}, \"UNIT_PRICE\": {unit_price}, \\\n",
    "\"DISCOUNT\": {discount}, \"SUPPLY_COST\": {supply_cost}, \"TAX\": {tax}, \\\n",
    "\"LAST_MODIFIED_TIMESTAMP\": \"{last_modified_timestamp.isoformat()}\"'+\"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:28:14.738863Z",
     "start_time": "2019-05-15T22:28:14.720858Z"
    }
   },
   "outputs": [],
   "source": [
    "#create new local file.\n",
    "def drop_file(i):\n",
    "    startDate = datetime.datetime(2018, 1, 1,0,0)+ datetime.timedelta(hours=i)\n",
    "    filename='/tmp/'+id_generator()+'.txt'\n",
    "    with open(filename, 'w+') as f:\n",
    "        for k in range(i*1031500,(i+1)*1031500):\n",
    "            gen_order(f,k,i,startDate)\n",
    "    return (filename,startDate)\n",
    "\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:28:17.024931Z",
     "start_time": "2019-05-15T22:28:17.006179Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket='<bucket>'\n",
    "t='SALES_ORDER_FACT'\n",
    "\n",
    "# Place file in S3 datalake\n",
    "def generate_file(i):\n",
    "    f,startDate=drop_file(i)\n",
    "    df=pd.read_json(f,lines=True)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    s3Location='s3://{0}/cdc/{1}/year={2}/month={3}/day={4}/hour={5}'\\\n",
    "        .format(bucket,t,startDate.year,startDate.month,startDate.day,startDate.hour)\n",
    "    s3 = S3FileSystem() \n",
    "    pq.write_to_dataset(table, s3Location, filesystem=s3, use_dictionary=True, compression='snappy')\n",
    "    os.remove(f)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:36:20.714248Z",
     "start_time": "2019-05-15T22:36:19.481828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  i|\n",
      "+---+\n",
      "|129|\n",
      "|139|\n",
      "|120|\n",
      "|140|\n",
      "|114|\n",
      "|141|\n",
      "|135|\n",
      "|143|\n",
      "|134|\n",
      "|136|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 33224)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "----------------------------------------"
     ]
    }
   ],
   "source": [
    "n=744\n",
    "df=sc.parallelize(range(n)).map(lambda x:Row(x)).toDF([\"i\"])\n",
    "df=df.repartition(n)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:44:44.526258Z",
     "start_time": "2019-05-15T22:36:23.801652Z"
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.map(lambda x:generate_file(x[\"i\"])).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
