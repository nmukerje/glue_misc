{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook applies changes to a table in a datalake, rollbacks the changes and reapplies the changes.\n",
    "\n",
    "Points to Note:\n",
    "\n",
    "1. Each change records a changeId corresponding to the epoch time of the change e.g. changeId=1558676810\n",
    "2. If the table location is in \"s3://&lt;bucket>/&lt;prefix>/TABLE\", the changes to each table are stored in the Changes location in \"s3://&lt;bucket>/&lt;prefix>/TABLE_changes\".\n",
    "3. For each change the change file is a json file containing the file and version of each change.\n",
    "4. A rollback simply deletes the version corresponding to a change, and deletes the record of the change from the Changes location.\n",
    "5. In addition, each change also stages the changed files in \"s3://&lt;bucket>/&lt;prefix>/TABLE_staging\"\n",
    "6. The changed files contain the original file in the prefix hence it is trivial to apply a change.\n",
    "7. A change can be applied if it is the last change in the Staging location but is not recorded in the Changes location.\n",
    "8. A rollback to a previous changeId deletes all changes after the changeId and all file versions after the changeId is deleted.\n",
    "\n",
    "\n",
    "Boto3 is required to be installed on EMR:\n",
    "```\n",
    "$> wget https://bootstrap.pypa.io/get-pip.py\n",
    "$> sudo python3 get-pip.py\n",
    "$> sudo /usr/local/bin/pip3 install boto3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo '{\"kernel_python_credentials\" : {\"url\": \"http://172.31.35.13:8998/\"}, \"session_configs\": \n",
    "{\"executorMemory\": \"2g\",\"executorCores\": 2,\"numExecutors\":4}}' > ~/.sparkmagic/config.json\n",
    "less ~/.sparkmagic/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set executor size to a single core and 7 GB RAM. Note that the ratio 1:7 matches the instances used in the cluster - r5.2xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-23T19:23:12.023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '8000M', 'executorMemory': '6000M', 'executorCores': 1, 'conf': {'spark.task.maxFailures': '10', 'spark.executor.memoryOverhead': '1G'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"8000M\",\"executorMemory\": \"6000M\", \"executorCores\": 1, \"conf\":  { \"spark.task.maxFailures\":\"10\",\"spark.executor.memoryOverhead\":\"1G\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>163</td><td>application_1555125850663_0167</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-13.us-west-2.compute.internal:20888/proxy/application_1555125850663_0167/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-36-147.us-west-2.compute.internal:8042/node/containerlogs/container_1555125850663_0167_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from __future__ import print_function\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import IntegerType\n",
    "from urllib.parse import unquote\n",
    "import random\n",
    "import calendar\n",
    "import boto3\n",
    "import time\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions used to generate data, run tests. etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T08:41:45.011552Z",
     "start_time": "2019-05-16T08:41:44.995591Z"
    }
   },
   "outputs": [],
   "source": [
    "max_n=767435999\n",
    "udf = UserDefinedFunction(lambda x: random.randint(0,100), IntegerType())    \n",
    "\n",
    "def generate_delete_data(n,key):\n",
    "    ids=[str(random.randrange(0, max_n)) for k in range(n)]\n",
    "    return (ids,sc.parallelize(ids).map(lambda x:Row(x)).toDF([key]))\n",
    "\n",
    "def generate_update_data(table,n,key):\n",
    "    ids=[str(random.randrange(0, max_n)) for k in range(n)]\n",
    "    sql=\"SELECT * FROM %s WHERE %s IN (%s)\"%(table,key,\",\".join(ids))\n",
    "    #print (\"SQL : \"+sql)\n",
    "    to_update_df=spark.sql(sql)\n",
    "    name='QUANTITY'\n",
    "    paths=spark.sql(\"SELECT input_file_name() as INPUT FROM %s limit 1\"%(table)).rdd.map(lambda x:x[0]).take(1)\n",
    "    input_df=spark.read.load(paths)\n",
    "    to_update_df_1 = to_update_df.select(*[udf(column).alias(name) if column == name else column for column in input_df.columns])\n",
    "    return (ids,to_update_df_1)\n",
    "\n",
    "def get_delete_count(table,key,ids):\n",
    "    sql=\"SELECT count(*) FROM %s WHERE %s IN (%s)\"%(table,key,\",\".join(ids))\n",
    "    count=spark.sql(sql).rdd.map(lambda x:x[0]).take(1)[0]\n",
    "    print ('Count of matching records : %d'%(count))\n",
    "\n",
    "def get_update_count(table,key,ids):\n",
    "    sql=\"SELECT count(*), SUM(QUANTITY) FROM %s WHERE %s IN (%s)\"%(table,key,\",\".join(ids))\n",
    "    count=spark.sql(sql).rdd.map(lambda x:(x[0],x[1])).take(1)\n",
    "    print ('Count of matching records : %d'%(count[0][0]))\n",
    "    print ('Sum of Quantity : %d'%(count[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions\n",
    "\n",
    "The core operations implemented are:\n",
    "    \n",
    "   1. **apply_to_datalake** : Applies Deletes and Updates to a table in the datalake.\n",
    "   2. **rollback** : Rollbacks the most recent change to a database.\n",
    "   3. **rollback_to_change** : Rollbacks to a previous change.\n",
    "   4. **apply_change** : Re-applies a previous change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name='cdc.sales_order_fact'\n",
    "primary_key='order_id'\n",
    "\n",
    "def get_table_location(t):\n",
    "    tables={\"cdc.sales_order_fact\":\"s3://<bucket>/cdc/SALES_ORDER_FACT\"}\n",
    "    return tables[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_file(src_bucket,target_bucket,key):    \n",
    "    target_url=key.split('/')[3].split(\"input=\")[1]\n",
    "    target_url = unquote(target_url)\n",
    "    target_key=target_url.split(\"s3://%s/\"%(target_bucket))[1]\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    resp=s3.copy_object(Bucket=target_bucket, CopySource=src_bucket+\"/\"+key, Key=target_key)\n",
    "    return (target_key,resp['VersionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies changes to the datalake\n",
    "def apply_to_datalake(table,key,delete_df,update_df):\n",
    "    \n",
    "    try:\n",
    "        ## Step 1: Get files impacted\n",
    "        table_location=get_table_location(table)\n",
    "        values=update_df.select(key).union(delete_df.select(key)).rdd.map(lambda x:x[0]).collect()\n",
    "        sql=\"SELECT %s, input_file_name() as input FROM %s WHERE %s IN (%s)\"%(key,table,key,\",\".join(list(set(values))))\n",
    "        #print (\"SQL : \"+sql)\n",
    "        start_time = time.time()\n",
    "        result=spark.sql(sql).rdd.map(lambda x:(x[0],x[1])).collect()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        keyfilemap={x[0]:x[1] for x in result}\n",
    "        paths=list(set([x[1] for x in result]))\n",
    "        print (\"Identified impacted files in %f seconds\"%(elapsed_time))\n",
    "        assert (len(paths) > 0),\"No records matched the keys.\"\n",
    "        print (\"%d files impacted\"%(len(paths)))\n",
    "    \n",
    "        ## Step 2: Read the data from those files using Spark/Glue.\n",
    "        input_df=spark.read.load(paths)\n",
    "        #input_record_count = input_df.count()\n",
    "        #print (\"Impacted Record Count : %d\"%(input_record_count))\n",
    "        # Add the Input filename as a column\n",
    "        input_df=input_df.withColumn(\"INPUT\", input_file_name())\n",
    "        #filename=input_df.select(\"INPUT\").rdd.map(lambda x: x[0]).take(1)[0]\n",
    "        #assert (len(filename) > 0),\"Input Filename not populated.\"\n",
    "    \n",
    "        ## Step 3: Join with the incremental data to filter out deletes and updates\n",
    "        keyfilemaps = sc.broadcast(keyfilemap)\n",
    "        lookup_file_udf = UserDefinedFunction(lambda x: keyfilemaps.value[x], StringType()) \n",
    "        update_df=update_df.withColumn('INPUT', lookup_file_udf(f.col(key)))\n",
    "        deleted_filter=\"ORDER_ID NOT IN (%s)\"%\",\".join(values)\n",
    "        changed_df=input_df.filter(deleted_filter)\n",
    "        #changed_record_count= changed_df.count()\n",
    "        #updated_record_count= update_df.count()\n",
    "        result_df=changed_df.union(update_df).cache()\n",
    "        #print (\"Records to Remove: %d\"%(input_record_count-changed_record_count))\n",
    "        #print (\"Records to Update: %d\"%(updated_record_count))\n",
    "        #expected_record_count=changed_record_count+updated_record_count\n",
    "        #print (\"Expected Result Record Count: %d\"%(expected_record_count))\n",
    "\n",
    "        ## Step 4: Write out new files back out to staging location in S3\n",
    "        change_id=str(calendar.timegm(time.gmtime()))\n",
    "        staging_location=\"%s_staging/changeId=%s\"%(table_location,change_id)\n",
    "        print (\"change_id : %s\"%(change_id))\n",
    "        partition_columns=[\"input\"]\n",
    "        start_time = time.time()\n",
    "        result_df.repartition(*partition_columns).write.partitionBy(partition_columns).parquet(staging_location)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print (\"Staged files in %f seconds\"%(elapsed_time))\n",
    "        #print (\"Staging Location: %s\"%(staging_location))\n",
    "    \n",
    "        ## Step 5: Run a parallel job to replace the old files with the new files.\n",
    "        apply_change(table,change_id)\n",
    "    \n",
    "        ## Validate counts\n",
    "        #input_record_count = input_df.count()\n",
    "        #print (\"Final Record Count : %d\"%(input_record_count))\n",
    "        #assert (input_record_count == expected_record_count),\"Final Record Counts do not match.\"\n",
    "        return change_id\n",
    "    except AssertionError:\n",
    "        print (\"Error: \")\n",
    "        logging.error(\"Assertion Exception : \", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollback(table,change_id):\n",
    "    table_location=get_table_location(table)\n",
    "    change_key=\"%s_changes/changeId=%s\"%(table_location,change_id)\n",
    "    print (change_key)\n",
    "    get_last_modified = lambda obj: int(obj['LastModified'].strftime('%s'))\n",
    "    start_time = time.time()\n",
    "    result=[]\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name=change_key.split('/')[2]\n",
    "    prefix='/'.join(change_key.split('/')[3:])\n",
    "    resp = s3.get_object(Bucket=bucket_name, Key=prefix) \n",
    "    changes = resp[\"Body\"].read().decode()\n",
    "    files=json.loads(changes)\n",
    "    for k,v in files.items():\n",
    "        # delete the versions\n",
    "        resp=s3.delete_object(Bucket=bucket_name, Key=k, VersionId=v)\n",
    "        respCode=resp['ResponseMetadata']['HTTPStatusCode']\n",
    "        result.append(respCode)\n",
    "    # delete the change record as well\n",
    "    resp=s3.delete_object(Bucket=bucket_name, Key=prefix)\n",
    "    #print(resp)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print (\"Change %s rolled back in %f seconds\"%(change_id,elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_change(table,change_id):\n",
    "    table_location=get_table_location(table)\n",
    "    s3 = boto3.client('s3')\n",
    "    staging_location=\"%s_staging/changeId=%s\"%(table_location,change_id)\n",
    "    bucket=staging_location.split('/')[2]\n",
    "    prefix='/'.join(staging_location.split('/')[3:])\n",
    "    resp = s3.list_objects_v2(Bucket=bucket,Prefix=prefix)\n",
    "    keys=[obj['Key'] for obj in resp['Contents'] if obj['Size'] > 0]\n",
    "    files=sc.parallelize(keys).map(lambda x: Row(x)).toDF([\"keys\"])\n",
    "    files=files.repartition(files.count())\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # replace original files\n",
    "    results=files.rdd.map(lambda x:replace_file(bucket,bucket,x[\"keys\"])).collect()\n",
    "    j={k[0]:k[1] for k in results}\n",
    "    j=json.dumps(j, ensure_ascii=False)\n",
    "    change_key=\"%s_changes/changeId=%s\"%(table_location,change_id)\n",
    "    change_key='/'.join(change_key.split('/')[3:])\n",
    "    #print (change_key)\n",
    "    \n",
    "    # record the change id\n",
    "    resp=s3.put_object(Body=j, Bucket=bucket, Key=change_key)\n",
    "    #print (resp['ResponseMetadata']['HTTPStatusCode'])\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print (\"Applied changes in %f seconds\"%(elapsed_time))\n",
    "    assert(len(results)==len(keys)),\"Count of files written does not match.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some Delete and Update records\n",
    "\n",
    "Let's Generate some Delete and Update records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 54628)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "----------------------------------------"
     ]
    }
   ],
   "source": [
    "# Let's generate some random data to delete and update\n",
    "n=1000\n",
    "delete_ids,cdc_delete_df=generate_delete_data(n,primary_key)\n",
    "update_ids,cdc_update_df=generate_update_data(table_name,n,primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of matching records : 2905\n",
      "Count of matching records : 3080\n",
      "Sum of Quantity : 154122"
     ]
    }
   ],
   "source": [
    "# Let's get the counts of records to be deleted and values before \n",
    "# records are updated.\n",
    "get_delete_count(table_name,primary_key,delete_ids)\n",
    "get_update_count(table_name,primary_key,update_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the changes\n",
    "\n",
    "Let's apply the changes to our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified impacted files in 63.430865 seconds\n",
      "693 files impacted\n",
      "change_id : 1559349039\n",
      "Staged files in 1368.523836 seconds\n",
      "Applied changes in 68.640070 seconds\n",
      "change_id=1559349039"
     ]
    }
   ],
   "source": [
    "# Applying the changes\n",
    "change_id=apply_to_datalake(table_name,primary_key,cdc_delete_df,cdc_update_df)\n",
    "print ('change_id=%s'%(change_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the results:\n",
    "\n",
    "Let's run some queries to validate the results after the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of matching records : 0\n",
      "Count of matching records : 3080\n",
      "Sum of Quantity : 154704"
     ]
    }
   ],
   "source": [
    "# Let's get the counts of matching records after deletion  \n",
    "# and values after records are updated.\n",
    "get_delete_count(table_name,primary_key,delete_ids)\n",
    "get_update_count(table_name,primary_key,update_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected:\n",
    "1. The matching records for the Deleted Ids is 0, so the records are deleted.\n",
    "2. The count of matching records for the Updated Ids remain the same, but the sum of the Quantity field which we are updating is changed.\n",
    "\n",
    "## Rollback the changes\n",
    "\n",
    "Let's rollback the changes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://<bucket>/cdc/SALES_ORDER_FACT_changes/changeId=1559349039\n",
      "Change 1559349039 rolled back in 17.296286 seconds"
     ]
    }
   ],
   "source": [
    "rollback(table_name,change_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes were rolled back. The rollback is multiple lightweight S3 delete operations on the versions of a file corresponding to changeIds.\n",
    "\n",
    "## Validating the Rollback\n",
    "\n",
    "Let's now validate that the counts remain the same before the changes were applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of matching records : 2905\n",
      "Count of matching records : 3080\n",
      "Sum of Quantity : 154122"
     ]
    }
   ],
   "source": [
    "get_delete_count(table_name,primary_key,delete_ids)\n",
    "get_update_count(table_name,primary_key,update_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the counts match the counts before the changes.\n",
    "\n",
    "## Reapply a change\n",
    "\n",
    "Let us reapply the last change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied changes in 67.277989 seconds"
     ]
    }
   ],
   "source": [
    "apply_change(table_name,change_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of matching records : 0\n",
      "Count of matching records : 3080\n",
      "Sum of Quantity : 154704"
     ]
    }
   ],
   "source": [
    "# Let's recompute the counts.\n",
    "get_delete_count(table_name,primary_key,delete_ids)\n",
    "get_update_count(table_name,primary_key,update_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
